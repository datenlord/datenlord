name: CI
on:
  pull_request:
    branches: [master]
  schedule: [cron: "0 */4 * * *"]

env:
  CI_RUST_TOOLCHAIN: 1.47.0
  CONTROLLER_APP_LABEL: csi-controller-datenlord
  CONTROLLER_CONTAINER_NAME: datenlord-controller-plugin
  DATENLORD_CSI_IMAGE: datenlord/csiplugin:e2e_test
  DATENLORD_NAMESPACE: csi-datenlord
  FUSE_CONTAINER_NAME: datenlord-fuse
  FUSE_MOUNT_PATH: /var/opt/datenlord-data
  K8S_CONFIG: k8s.e2e.config
  K8S_VERSION: v1.19.0
  KIND_VERSION: v0.9.0
  NODE_APP_LABEL: csi-nodeplugin-datenlord
  NODE_CONTAINER_NAME: datenlord-node-plugin
  MINIKUBE_VERSION: v1.13.0

jobs:
  outdated:
    name: Outdated
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - uses: actions-rs/install@v0.1
        with:
          crate: cargo-outdated
          version: latest
      - run: cargo outdated

  audit:
    name: Audit
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
      with:
        crate: cargo-audit
        use-tool-cache: true
    - uses: actions-rs/audit-check@v1.2.0
      with:
        token: ${{ secrets.GITHUB_TOKEN }}

  check:
    name: Check
    runs-on: ubuntu-latest
    steps:
      - run: sudo apt install -y libprotobuf-dev protobuf-compiler
      - uses: actions/checkout@v2
      - uses: actions-rs/toolchain@v1
        with:
          profile: default
          toolchain: ${{ env.CI_RUST_TOOLCHAIN }}
          override: true
      - uses: actions-rs/cargo@v1
        with:
          command: check

  e2e:
    name: E2E-Test
    runs-on: ubuntu-latest
    steps:
      #- name: Install MicroK8s
      #  uses: balchua/microk8s-actions@v0.1.2
      #  with:
      #    channel: '1.18/stable'
      #    rbac: 'true'
      #    dns: 'true'
      #- name: Allow previleged container in MicroK8s
      #  run: |
      #    kubectl get no
      #    echo "--allow-privileged=true" | sudo tee -a /var/snap/microk8s/current/args/kubelet /var/snap/microk8s/current/args/kube-apiserver
      #    sudo systemctl restart snap.microk8s.daemon-kubelet.service
      #    sudo systemctl restart snap.microk8s.daemon-apiserver.service
      #    sudo cat /var/snap/microk8s/current/args/kubelet /var/snap/microk8s/current/args/kube-apiserver
      #    sudo netstat -lntp
      #- name: Prepare docker environment
      #  uses: crazy-max/ghaction-docker-buildx@v3
      #  with:
      #    buildx-version: latest
      #    qemu-version: latest
      - name: Set SSH config
        run: |
          sudo rm -rf ~/.ssh/
          ssh-keygen -N '' -f ~/.ssh/id_rsa
          cp ~/.ssh/id_rsa.pub ~/.ssh/authorized_keys
          cat ~/.ssh/authorized_keys
          export HOSTNAME=`hostname`
          export WHO=`whoami`
          export HOSTIP=`ifconfig eth0 | grep 'inet ' | awk '{print $2}'`
          cat >>~/.ssh/config <<END
          Host thisrunner
            HostName $HOSTNAME
            User $WHO
            IdentityFile $HOME/.ssh/id_rsa
            StrictHostKeyChecking no
            UserKnownHostsFile /dev/null
          END
          chmod 755 $HOME
          sudo service ssh restart
          sudo service ssh status
          ssh -vvv thisrunner
          exit
      #- name: SSH keygen for e2e test
      #  run: |
      #    sudo rm -rf ~/.ssh/
      #    ssh-keygen -N '' -f ~/.ssh/id_rsa
      #    cp ~/.ssh/id_rsa.pub ~/.ssh/authorized_keys
      #    chmod 600 ~/.ssh/*
      #    export HOSTNAME=`hostname`
      #    export WHO=`whoami`
      #    export HOSTIP=`ifconfig eth0 | grep 'inet ' | awk '{print $2}'`
      #    cat >>~/.ssh/config <<END
      #    Host thisrunner
      #      HostName $HOSTNAME
      #      User $WHO
      #      IdentityFile $HOME/.ssh/id_rsa
      #      StrictHostKeyChecking no
      #      PasswordAuthentication no
      #      UserKnownHostsFile /dev/null
      #    END
      #    sudo service ssh restart
      #    sudo service ssh status
      #    cat ~/.ssh/config
      #    #cat /etc/ssh/sshd_config
      #    #cat /etc/ssh/ssh_config
      #    pwd
      #    ls -lsh /dev/tty
      #    ls -lsh ~/.ssh
      #    ls -lsh /home/runner/.ssh/id_rsa
      #    ls -lsh /home/runner/.ssh/id_rsa.pub
      #    ls -lsh /home/runner/.ssh/authorized_keys
      #    export HOSTIP=`ifconfig eth0 | grep 'inet ' | awk '{print $2}'`
      #    echo "IP: $HOSTIP"
      #    ssh-keyscan -H $HOSTNAME,$HOSTIP >> ~/.ssh/known_hosts
      #    cat ~/.ssh/known_hosts
      #    eval "$(ssh-agent -s)"
      #    ssh-add -l
      #    ssh-add -L
      #    ssh-add -k ~/.ssh/id_rsa
      #    ssh-add -l
      #    ssh-add -L
      #    #ssh -vvv $HOSTIP ls
      #    #ssh -vvv registry ls
      #    #ssh -vvv -o StrictHostKeyChecking=no -i ~/.ssh/id_rsa 127.0.0.1
      #    #ssh -vvv thisrunner 'ls -lsh'
      - name: Check out code
        uses: actions/checkout@v2
      - name: Setup Minikube
        uses: manusa/actions-setup-minikube@v2.0.0
        with:
          #minikube version: 'v1.13.0'
          #kubernetes version: 'v1.19.0'
          minikube version: ${{ env.MINIKUBE_VERSION }}
          kubernetes version: ${{ env.K8S_VERSION }}
          github token: ${{ secrets.GITHUB_TOKEN }}
      #- name: Deploy minikube
      #  uses: opsgang/ga-setup-minikube@v0.1.1
      #  with:
      #    minikube-version: 1.12.3
      #    k8s-version: 1.18.8
      #- name: Test deploy DatenLord CSI plugin to K8s
      #  run: |
      #    kubectl cluster-info
      #    kubectl get pods -A
      #    docker pull quay.io/k8scsi/csi-node-driver-registrar:v1.3.0
      #    docker pull quay.io/k8scsi/livenessprobe:v2.0.0
      #    docker pull quay.io/k8scsi/csi-attacher:v2.2.0
      #    docker pull quay.io/k8scsi/csi-provisioner:v1.6.0
      #    docker pull quay.io/k8scsi/csi-snapshotter:v2.1.1
      #    docker pull quay.io/k8scsi/csi-resizer:v0.5.0
      #    docker pull gcr.io/etcd-development/etcd:latest
      #    docker pull datenlord/csiplugin:latest
      #    kubectl apply -f csiplugin_k8s.yaml
      #    kubectl get csidriver
      #    while [[ $(kubectl get pods -l app=csi-controller-datenlord -A -o 'jsonpath={..status.conditions[?(@.type=="Ready")].status}') != "True" ]]; do kubectl get pods -A -o wide ; kubectl get nodes -o wide ; kubectl describe pod csi-controller-datenlord-0 -n csi-datenlord ; sleep 6 ; done
      #    while [[ $(kubectl get pods -l app=csi-nodeplugin-datenlord -A -o 'jsonpath={..status.conditions[?(@.type=="Ready")].status}') != "True" ]]; do kubectl get pods -A -o wide ; kubectl get nodes -o wide ; sleep 6 ; done
      #    kubectl get pods -A -o wide
      #    sudo netstat -lntp && ls -lsh
      - name: Prepare Rust environment
        uses: actions-rs/toolchain@v1
        with:
          profile: default
          toolchain: ${{ env.CI_RUST_TOOLCHAIN }}
          override: true
      - name: Install build dependencies
        run: sudo apt install -y cmake g++ libprotobuf-dev protobuf-compiler
      - name: Cargo build
        uses: actions-rs/cargo@v1
        with:
          command: build
      - name: Docker build csi plugin
        run: |
          export BUILD_DIR=/tmp/datenlord_container_build
          mkdir -p $BUILD_DIR
          cp target/debug/csi $BUILD_DIR
          docker build $BUILD_DIR --file ./Dockerfile --tag $DATENLORD_CSI_IMAGE
      - name: Deploy DatenLord CSI plugin to K8s
        run: |
          kubectl cluster-info
          kubectl get pods -A
          kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/master/client/config/crd/snapshot.storage.k8s.io_volumesnapshots.yaml
          kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/master/client/config/crd/snapshot.storage.k8s.io_volumesnapshotcontents.yaml
          kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/master/client/config/crd/snapshot.storage.k8s.io_volumesnapshotclasses.yaml
          kubectl apply -f csiplugin_k8s_minikube.yaml
          kubectl get csidriver
          kubectl get csinode
          kubectl get storageclass
          kubectl get volumesnapshotclass
          kubectl wait --for=condition=Ready pod -l app=$CONTROLLER_APP_LABEL -n $DATENLORD_NAMESPACE --timeout=60s
          kubectl wait --for=condition=Ready pod -l app=$NODE_APP_LABEL -n $DATENLORD_NAMESPACE --timeout=60s
          #kubectl wait --for=condition=Ready pod -l app=csi-controller-datenlord -n csi-datenlord --timeout=60s
          #kubectl wait --for=condition=Ready pod -l app=csi-nodeplugin-datenlord -n csi-datenlord --timeout=60s
          wget https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/master/deploy/kubernetes/snapshot-controller/rbac-snapshot-controller.yaml
          wget https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/master/deploy/kubernetes/snapshot-controller/setup-snapshot-controller.yaml
          sed -e 's/namespace\:\ default/namespace\:\ kube\-system/g' rbac-snapshot-controller.yaml > datenlord-rbac-snapshot-controller.yaml
          sed -e 's/namespace\:\ default/namespace\:\ kube\-system/g' setup-snapshot-controller.yaml > datenlord-setup-snapshot-controller.yaml
          kubectl apply -f datenlord-rbac-snapshot-controller.yaml
          kubectl apply -f datenlord-setup-snapshot-controller.yaml
          kubectl wait --for=condition=Ready pod -l app=snapshot-controller -n kube-system --timeout=60s
          kubectl get pods -A -o wide
          sudo netstat -lntp && ls -lsh
      - name: E2E Test
        run: |
          wget --quiet https://dl.k8s.io/$K8S_VERSION/kubernetes-test-linux-amd64.tar.gz
          #wget --quiet https://dl.k8s.io/v1.19.0/kubernetes-test-linux-amd64.tar.gz
          tar zxvf kubernetes-test-linux-amd64.tar.gz
          kubectl config view --raw > $K8S_CONFIG
          #kubernetes/test/bin/e2e.test -v=5 -ginkgo.debug -ginkgo.failFast -ginkgo.failOnPending -ginkgo.v -ginkgo.focus='External.Storage' -kubectl-path=`which kubectl` -kubeconfig=$K8S_CONFIG -storage.testdriver=datenlord-e2e-test.yaml -test.parallel=3
          kubernetes/test/bin/ginkgo -p -v -failFast -failOnPending -debug -focus='External.Storage' kubernetes/test/bin/e2e.test -- -v=5 -kubectl-path=`which kubectl` -kubeconfig=$PWD/$K8S_CONFIG -storage.testdriver=$PWD/datenlord-e2e-test.yaml -test.parallel=3
      - name: Print DatenLord logs
        if: ${{ failure() }}
        run: |
          CONTROLLER_POD_NAME=`kubectl get pod -l app=$CONTROLLER_APP_LABEL -n $DATENLORD_NAMESPACE -o jsonpath="{.items[0].metadata.name}"`
          echo "SHOW LOGS OF $CONTROLLER_CONTAINER_NAME IN $CONTROLLER_POD_NAME"
          kubectl logs $CONTROLLER_POD_NAME -n $DATENLORD_NAMESPACE -c $CONTROLLER_CONTAINER_NAME
          NODE_POD_NAME=`kubectl get pod -l app=$NODE_APP_LABEL -n $DATENLORD_NAMESPACE -o jsonpath="{.items[0].metadata.name}"`
          echo "SHOW LOGS OF $NODE_CONTAINER_NAME IN $NODE_POD_NAME"
          kubectl logs $NODE_POD_NAME -n $DATENLORD_NAMESPACE -c $NODE_CONTAINER_NAME
          #echo "SHOW LOGS OF $FUSE_CONTAINER_NAME IN $NODE_POD_NAME"
          #kubectl logs $NODE_POD_NAME -n $DATENLORD_NAMESPACE -c $FUSE_CONTAINER_NAME
      - name: Setup tmate session
        if: ${{ failure() }}
        uses: mxschmitt/action-tmate@v3

  test:
    name: CSI-Sanity-Test
    runs-on: ubuntu-latest
    #services:
    #  etcd:
    #    image: gcr.io/etcd-development/etcd:latest
    #    # Set health checks to wait until etcd has started
    #    options: >-
    #      --health-cmd "/usr/local/bin/etcdctl cluster-health"
    #      --health-interval 10s
    #      --health-timeout 5s
    #      --health-retries 5
    #      --entrypoint /usr/local/bin/etcd
    #    ports:
    #      - 2379:2379
    #      - 2380:2380
    steps:
      - name: Install dependencies
        run: sudo apt install -y cmake g++ libprotobuf-dev protobuf-compiler
      - name: Set up Docker
        id: buildx
        uses: docker/setup-buildx-action@v1
        with:
          version: latest
      - name: Check out code
        uses: actions/checkout@v2
      - name: Prepare Rust environment
        uses: actions-rs/toolchain@v1
        with:
          profile: default
          toolchain: ${{ env.CI_RUST_TOOLCHAIN }}
          override: true
      - name: Cargo build
        uses: actions-rs/cargo@v1
        with:
          command: build
      - name: Run etcd service
        run: |
          export ETCD_CONTAINER_NAME=etcd
          docker run -d --rm --net host --name $ETCD_CONTAINER_NAME gcr.io/etcd-development/etcd:latest
          docker ps
          docker logs $ETCD_CONTAINER_NAME
          sudo netstat -lntp
      - name: Enable bind_mounter permission
        run: |
          export BIND_MOUNTER=target/debug/bind_mounter
          sudo chown root:root $BIND_MOUNTER
          sudo chmod u+s $BIND_MOUNTER
          ls -lsh $BIND_MOUNTER
          export ETCD_END_POINT=http://127.0.0.1:2379
      - name: Cargo test
        uses: actions-rs/cargo@v1
        with:
          command: test
      - name: Cargo build
        uses: actions-rs/cargo@v1
        with:
          command: build
      - name: Enable bind_mounter permission again
        run: |
          export BIND_MOUNTER=target/debug/bind_mounter
          sudo chown root:root $BIND_MOUNTER
          sudo chmod u+s $BIND_MOUNTER
          ls -lsh $BIND_MOUNTER
      - name: Restart etcd service to clean up its data
        run: |
          export ETCD_CONTAINER_NAME=etcd
          docker kill $ETCD_CONTAINER_NAME
          docker run -d --rm --net host --name $ETCD_CONTAINER_NAME gcr.io/etcd-development/etcd:latest
          docker ps
          docker logs $ETCD_CONTAINER_NAME
          sudo netstat -lntp
      #- name: Docker build csi plugin
      #  run: export BUILD_DIR=/tmp/datenlord_container_build ; mkdir -p $BUILD_DIR && cp target/debug/csi $BUILD_DIR && docker build $BUILD_DIR --file ./Dockerfile --tag datenlord/csiplugin:latest
      #- name: Docker run csi controller and node seperately
      #  run: |
      #    docker run -d --rm -v /tmp:/tmp --net host --env RUST_LOG=csi=debug --name datenlord_csi_controller datenlord/csiplugin -n localhost -s unix:///tmp/controller.sock -p 50051 -r controller -e http://127.0.0.1:2379 --datadir /tmp/datenlord-data-dir
      #    docker run -d --rm -v /tmp:/tmp --net host --env RUST_LOG=csi=debug --name datenlord_csi_node datenlord/csiplugin -n localhost -s unix:///tmp/node.sock -p 50051 -r node -e http://127.0.0.1:2379 --datadir /tmp/datenlord-data-dir
      #- name: Docker run csi plugin
      #  run: docker run -d --rm -v /tmp:/tmp --net host --env RUST_LOG=debug --name datenlord_csi datenlord/csiplugin -n localhost -s unix:///tmp/csi.sock -p 50051 -r both -e http://127.0.0.1:2379 --datadir /tmp/datenlord-data-dir
      #- name: Debug Info
      #  run: sudo netstat -lntp ; docker ps ; docker logs etcd ; docker logs datenlord_csi_controller ; docker logs datenlord_csi_node ;
      #- name: Run csi-sanity test
      #  run: docker run --rm -v /tmp:/tmp --name sanity golang:latest sh -c "go get github.com/kubernetes-csi/csi-test/cmd/csi-sanity && \$GOPATH/bin/csi-sanity -csi.endpoint=/tmp/node.sock -csi.controllerendpoint=/tmp/controller.sock"
      - name: Check out csi-sanity code
        uses: actions/checkout@v2
        with:
          repository: kubernetes-csi/csi-test
          path: ./csi-test
      - name: Build csi-sanity code
        uses: cedrickring/golang-action@1.5.2
        env:
          PROJECT_PATH: "./csi-test/cmd/csi-sanity"
      - name: Run csi plugin and csi-sanity test
        run: |
          export RUST_LOG=csi=debug
          export CONTROLLER_SOCKET_FILE=/tmp/controller.sock
          export NODE_SOCKET_FILE=/tmp/node.sock
          export ETCD_END_POINT=http://127.0.0.1:2379
          target/debug/csi --endpoint=unix://$NODE_SOCKET_FILE --workerport=0 --nodeid=localhost --drivername=io.datenlord.csi.plugin --datadir=/tmp/datenlord_data_dir --runas=node --etcd=$ETCD_END_POINT &
          target/debug/csi --endpoint=unix://$CONTROLLER_SOCKET_FILE --workerport=0 --nodeid=localhost --drivername=io.datenlord.csi.plugin --datadir=/tmp/datenlord_data_dir --runas=controller --etcd=$ETCD_END_POINT &
          ./csi-test/cmd/csi-sanity/csi-sanity -csi.endpoint=$NODE_SOCKET_FILE -csi.controllerendpoint=$CONTROLLER_SOCKET_FILE
      - name: Setup tmate session
        if: ${{ failure() }}
        uses: mxschmitt/action-tmate@v3

  fmt:
    name: Format
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - uses: actions-rs/toolchain@v1
        with:
          profile: default
          toolchain: ${{ env.CI_RUST_TOOLCHAIN }}
          override: true
      - uses: actions-rs/cargo@v1
        with:
          command: fmt
          args: --all -- --check

  clippy:
    name: Clippy
    runs-on: ubuntu-latest
    steps:
      - run: sudo apt install -y cmake g++ libprotobuf-dev protobuf-compiler
      - uses: actions/checkout@v2
      - uses: actions-rs/toolchain@v1
        with:
          profile: default
          toolchain: ${{ env.CI_RUST_TOOLCHAIN }}
          override: true
      - uses: actions-rs/cargo@v1
        with:
          command: clippy
          args: --all-targets --all-features -- -D warnings

  kind-e2e:
    name: Kind-E2E-Test
    runs-on: ubuntu-latest
    steps:
      - name: Check out code
        uses: actions/checkout@v2
      - name: Setup go
        uses: actions/setup-go@v1
        with:
          go-version: 1.13
      - name: Set up Docker
        id: buildx
        uses: docker/setup-buildx-action@v1
        with:
          version: latest
      - name: Install Kind
        run: |
          curl -Lo ./kind https://kind.sigs.k8s.io/dl/$KIND_VERSION/kind-linux-amd64
          chmod +x ./kind
          sudo mv ./kind /usr/local/bin
      - name: Create Kind Cluster
        run: |
          kind create cluster --config kind-config.yaml
      - name: Setup SSH
        run: |
          cat >>/tmp/setup.sh <<'END'
          NODES_IP="$(kubectl get nodes -A -o wide | awk 'FNR > 2 {print $6}')"
          NODES="$(kubectl get nodes -A -o wide | awk 'FNR > 2 {print $1}')"
          for node in ${NODES}; do
            USER="$(whoami)"
            docker exec ${node} apt-get update
            docker exec ${node} apt-get install -y ssh sudo
            docker exec ${node} systemctl start sshd
            docker exec ${node} useradd -m ${USER}
            docker exec ${node} usermod -aG sudo ${USER}
            echo "${USER} ALL=(ALL) NOPASSWD:ALL" > /tmp/${USER}
            docker cp /tmp/${USER} ${node}:/etc/sudoers.d/${USER}
            docker exec ${node} chown root:root /etc/sudoers.d/${USER}
  
            docker exec ${node} mkdir /home/${USER}/.ssh
            docker exec ${node} ls -al /home/${USER}/
            docker cp ${HOME}/.ssh/id_rsa.pub ${node}:/home/${USER}/.ssh/authorized_keys
            docker exec ${node} chown ${USER}:${USER} /home/${USER}/ -R
          done
          for ip in ${NODES_IP}; do
              ssh-keyscan -H $ip >> ${HOME}/.ssh/known_hosts
          done
          END

          rm -rf $HOME/.ssh/
          ssh-keygen -N '' -f ~/.ssh/id_rsa
          /bin/bash /tmp/setup.sh

      - name: Prepare Rust environment
        uses: actions-rs/toolchain@v1
        with:
          profile: default
          toolchain: ${{ env.CI_RUST_TOOLCHAIN }}
          override: true
      - name: Install build dependencies
        run: sudo apt install -y cmake g++ libprotobuf-dev protobuf-compiler
      - name: Cargo build
        uses: actions-rs/cargo@v1
        with:
          command: build
      - name: Docker build csi plugin
        run: |
          export BUILD_DIR=/tmp/datenlord_container_build
          mkdir -p $BUILD_DIR
          cp target/debug/csi $BUILD_DIR
          docker build $BUILD_DIR --file ./Dockerfile --tag $DATENLORD_CSI_IMAGE
          kind load docker-image $DATENLORD_CSI_IMAGE
      - name: Deploy DatenLord CSI plugin to K8s
        run: |
          kubectl cluster-info
          kubectl get pods -A
          kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/master/client/config/crd/snapshot.storage.k8s.io_volumesnapshots.yaml
          kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/master/client/config/crd/snapshot.storage.k8s.io_volumesnapshotcontents.yaml
          kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/master/client/config/crd/snapshot.storage.k8s.io_volumesnapshotclasses.yaml
          kubectl apply -f csiplugin_k8s.yaml
          kubectl get csidriver
          kubectl get csinode
          kubectl get storageclass
          kubectl get volumesnapshotclass
          kubectl wait --for=condition=Ready pod -l app=$CONTROLLER_APP_LABEL -n $DATENLORD_NAMESPACE --timeout=60s
          kubectl wait --for=condition=Ready pod -l app=$NODE_APP_LABEL -n $DATENLORD_NAMESPACE --timeout=60s
          wget https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/master/deploy/kubernetes/snapshot-controller/rbac-snapshot-controller.yaml
          wget https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/master/deploy/kubernetes/snapshot-controller/setup-snapshot-controller.yaml
          sed -e 's/namespace\:\ default/namespace\:\ kube\-system/g' rbac-snapshot-controller.yaml > datenlord-rbac-snapshot-controller.yaml
          sed -e 's/namespace\:\ default/namespace\:\ kube\-system/g' setup-snapshot-controller.yaml > datenlord-setup-snapshot-controller.yaml
          kubectl apply -f datenlord-rbac-snapshot-controller.yaml
          kubectl apply -f datenlord-setup-snapshot-controller.yaml
          kubectl wait --for=condition=Ready pod -l app=snapshot-controller -n kube-system --timeout=60s
          kubectl get pods -A -o wide
          sudo netstat -lntp && ls -lsh
      - name: E2E Test
        run: |
          wget --quiet https://dl.k8s.io/$K8S_VERSION/kubernetes-test-linux-amd64.tar.gz
          tar zxvf kubernetes-test-linux-amd64.tar.gz
          kubectl config view --raw > $K8S_CONFIG
          #kubernetes/test/bin/e2e.test -v=5 -ginkgo.debug -ginkgo.failFast -ginkgo.failOnPending -ginkgo.v -ginkgo.focus='External.Storage' -ginkgo.skip='.*Dynamic PV.*filesystem.*should access to two volumes with the same volume mode and retain data across pod recreation on .* node|.*Dynamic PV.*default fs.*should allow exec of files on the volume' -kubectl-path=`which kubectl` -kubeconfig=$K8S_CONFIG -storage.testdriver=datenlord-e2e-test.yaml -test.parallel=3
          kubernetes/test/bin/ginkgo -p -v -failFast -failOnPending -debug -focus='External.Storage' -skip='.*Dynamic PV.*filesystem.*should access to two volumes with the same volume mode and retain data across pod recreation on .* node|.*Dynamic PV.*default fs.*should allow exec of files on the volume' kubernetes/test/bin/e2e.test -- -v=5 -kubectl-path=`which kubectl` -kubeconfig=$PWD/$K8S_CONFIG -storage.testdriver=$PWD/datenlord-e2e-test.yaml -test.parallel=3
      - name: Print DatenLord logs
        if: ${{ failure() }}
        run: |
          CONTROLLER_POD_NAME=`kubectl get pod -l app=$CONTROLLER_APP_LABEL -n $DATENLORD_NAMESPACE -o jsonpath="{.items[0].metadata.name}"`
          echo "SHOW LOGS OF $CONTROLLER_CONTAINER_NAME IN $CONTROLLER_POD_NAME"
          kubectl logs $CONTROLLER_POD_NAME -n $DATENLORD_NAMESPACE -c $CONTROLLER_CONTAINER_NAME
          NODE_POD_NAME=`kubectl get pod -l app=$NODE_APP_LABEL -n $DATENLORD_NAMESPACE -o jsonpath="{.items[0].metadata.name}"`
          echo "SHOW LOGS OF $NODE_CONTAINER_NAME IN $NODE_POD_NAME"
          kubectl logs $NODE_POD_NAME -n $DATENLORD_NAMESPACE -c $NODE_CONTAINER_NAME
          #echo "SHOW LOGS OF $FUSE_CONTAINER_NAME IN $NODE_POD_NAME"
          #kubectl logs $NODE_POD_NAME -n $DATENLORD_NAMESPACE -c $FUSE_CONTAINER_NAME
      - name: Setup tmate session
        if: ${{ failure() }}
        uses: mxschmitt/action-tmate@v3
#  coverage:
#    name: Coverage
#    runs-on: ubuntu-latest
#    steps:
#      - name: Checkout repository
#        uses: actions/checkout@v2
#
#      - name: Install stable toolchain
#        uses: actions-rs/toolchain@v1
#        with:
#          profile: minimal
#          toolchain: ${{ env.CI_RUST_TOOLCHAIN }}
#          override: true
#
#      - name: Run cargo-tarpaulin
#        uses: actions-rs/tarpaulin@v0.1
#        with:
#          version: '0.9.0'
#          args: '-- --test-threads 1'
#
#      - name: Upload to codecov.io
#        uses: codecov/codecov-action@v1.0.2
#        with:
#          token: ${{secrets.CODECOV_TOKEN}}
#
#      - name: Archive code coverage results
#        uses: actions/upload-artifact@v1
#        with:
#          name: code-coverage-report
#          path: cobertura.xml
